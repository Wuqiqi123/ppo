{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Gradient\n",
    "To maximize the objective function $J(\\theta)$\n",
    "    $$\n",
    "    \\underset{\\theta}{\\operatorname{arg max}}J(\\theta) = \\underset{\\theta}{\\operatorname{arg max}} \\mathbb{E}_{s \\sim d_\\pi}\\left [ V_\\pi(s) \\right ] \n",
    "    $$\n",
    "we can use the policy gradient theorem:\n",
    "$$\n",
    "\\nabla_\\theta J(\\theta)  = \\mathbb{E}_{s \\sim d_\\pi}\\left [\\mathbb{E}_{a \\sim \\pi_{\\theta}(\\cdot \\vert s)} [Q_\\pi(s, a) \\nabla_\\theta \\ln \\pi_\\theta(a \\vert s)] \\right ] \n",
    "$$\n",
    "Sample from $a \\sim \\pi_{\\theta}(\\cdot \\vert s)$ and a monte carlo estimation of $\\nabla_\\theta J(\\theta)$ is:\n",
    "$$\n",
    "\\boldsymbol{g}_\\theta(s, a) \\doteq Q_{\\pi_\\theta}(s, a) \\nabla_\\theta \\ln \\pi_\\theta(a \\vert s)\n",
    "$$\n",
    "\n",
    "## Generalization Advantage Estimation(GAE)\n",
    "![](./doc/gae_0.png)\n",
    "![](./doc/gae_1.png)\n",
    "![](./doc/gae_2.png)\n",
    "\n",
    "$$\n",
    "\\hat{A}_t^{GAE(\\gamma, \\lambda)} = \\sum_{l=0}^{\\infty} (\\gamma \\lambda)^l \\delta_{t+l} = \\delta_{t} + (\\gamma \\lambda) \\hat{A}_{t+1}^{GAE(\\gamma, \\lambda)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([9, 1])\n",
      "torch.Size([9, 1])\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "import torch    \n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, state_dim, hidden_dim = 64, dim_pred = 1):\n",
    "        super(Critic, self).__init__()\n",
    "        \n",
    "        self.proj_in = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_dim),\n",
    "            nn.Tanh(), \n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "        self.value_head = nn.Linear(hidden_dim, dim_pred)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = self.proj_in(state)\n",
    "        x = self.value_head(x)\n",
    "        return x\n",
    "\n",
    "num_samples = 9\n",
    "state_dim = 10\n",
    "critic = Critic(state_dim)\n",
    "state = torch.randn(num_samples, state_dim)\n",
    "rewards = torch.randn((num_samples, 1))\n",
    "dones = torch.zeros((num_samples, 1))\n",
    "dones[-1] = 1.0\n",
    "next_states = torch.randn(num_samples, state_dim)\n",
    "gamma = 0.99\n",
    "lambda_ = 0.95\n",
    "\n",
    "td_target = rewards + gamma * critic(next_states) * (1.0 - dones)\n",
    "td_delta = td_target - critic(state)\n",
    "print((gamma * critic(next_states)).shape)\n",
    "\n",
    "advantages = torch.zeros((num_samples, 1))\n",
    "advantages[-1] = td_delta[-1]\n",
    "for t in range(num_samples - 2, -1, -1):\n",
    "    advantages[t] = td_delta[t] + gamma * lambda_ * advantages[t + 1]\n",
    "\n",
    "print(advantages.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## The TRPO formula\n",
    "Importance sampling ratio:\n",
    "$$\n",
    "\\underset{\\theta}{\\operatorname{arg max}} \\mathbb{E}_{s}\\left [\\mathbb{E}_{a \\sim \\pi_{\\theta_\\text{old}}(\\cdot \\vert s)} [  \\frac{\\pi_\\theta(a \\vert s)}{\\pi_{\\theta_\\text{old}}(a \\vert s)} \\hat{A}_{\\theta_\\text{old}}(s, a) ] \\right]\n",
    "$$\n",
    "\n",
    "## The PPO Algorithm\n",
    "$$\n",
    "\\underset{\\theta}{\\operatorname{arg max}} \\mathbb{E}_{s}\\left [\\mathbb{E}_{a \\sim \\pi_{\\theta_\\text{old}}(\\cdot \\vert s)} [\\min(\\frac{\\pi_\\theta(a \\vert s)}{\\pi_{\\theta_\\text{old}}(a \\vert s)} \\hat{A}_{\\theta_\\text{old}}(s, a), \\text{clip}(\\frac{\\pi_\\theta(a \\vert s)}{\\pi_{\\theta_\\text{old}}(a \\vert s)}, 1 - \\epsilon, 1 + \\epsilon) \\hat{A}_{\\theta_\\text{old}}(s, a))]\\right]\n",
    "$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ppo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
