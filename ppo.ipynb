{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Gradient\n",
    "$$\n",
    "\\nabla_\\theta J(\\theta)  = \\mathbb{E}_{s \\sim d_\\pi}\\left [\\mathbb{E}_{a \\sim \\pi_{\\theta}(\\cdot \\vert s)} [Q_\\pi(s, a) \\nabla_\\theta \\ln \\pi_\\theta(a \\vert s)] \\right ] \n",
    "$$\n",
    "Sample from $a \\sim \\pi_{\\theta}(\\cdot \\vert s)$ and a monte carlo estimation of $\\nabla_\\theta J(\\theta)$ is:\n",
    "$$\n",
    "\\boldsymbol{g}_\\theta(s, a) \\doteq Q_{\\pi_\\theta}(s, a) \\nabla_\\theta \\ln \\pi_\\theta(a \\vert s)\n",
    "$$\n",
    "\n",
    "## Generalization Advantage Estimation(GAE)\n",
    "Substitute $Q_\\pi(s, a)$ with $A_\\pi(s, a)$, the advantage function:\n",
    "$$\n",
    "\\boldsymbol{g}_\\theta(s, a) \\doteq A_{\\pi_\\theta}(s, a) \\nabla_\\theta \\ln \\pi_\\theta(a \\vert s)\n",
    "$$\n",
    "\n",
    "## The TRPO formula\n",
    "Importance sampling ratio:\n",
    "$$\n",
    "J^\\text{TRPO} (\\theta) = \\mathbb{E}_{s}\\left [\\mathbb{E}_{a \\sim \\pi_{\\theta_\\text{old}}(\\cdot \\vert s)} [  \\frac{\\pi_\\theta(a \\vert s)}{\\pi_{\\theta_\\text{old}}(a \\vert s)} \\hat{A}_{\\theta_\\text{old}}(s, a) ] \\right]\n",
    "$$\n",
    "\n",
    "## The PPO Algorithm\n",
    "$$\n",
    "J^\\text{CLIP} (\\theta) = \\mathbb{E}_{a \\sim \\pi_{\\theta_\\text{old}}(\\cdot \\vert s)} [\\min(\\frac{\\pi_\\theta(a \\vert s)}{\\pi_{\\theta_\\text{old}}(a \\vert s)} \\hat{A}_{\\theta_\\text{old}}(s, a), \\text{clip}(r(\\theta), 1 - \\epsilon, 1 + \\epsilon) \\hat{A}_{\\theta_\\text{old}}(s, a))]\n",
    "$$\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
